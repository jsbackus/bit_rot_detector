#!/usr/bin/python

from __future__ import unicode_literals
import logging
import os
#from os.path import join
import stat
import hashlib
import argparse
import sqlite3
import operator
import time

###########
# Globals #
###########
table_names = { 'files': 'fp_files', 'dirs': 'fp_dirs' }

#############
# Functions #
#############
def parse_args():
    """Sets up the argparse object and attempts to parse the command-line.
    """
    
    # Create parser object
    parser = argparse.ArgumentParser(description=
                                     'Tool verify file integrity.')
    parser.add_argument('-l', '--log', nargs='?', 
                        default='', help='log to file instead of console')
    parser.add_argument('-q', '--quiet', action='store_true',
                        help='only display warnings and errors')
    parser.add_argument('-d', '--debug', action='store_true',
                        help='display debugging information')
    parser.add_argument('-p', '--prune', action='store_true',
                        help='Prunes directories and files from database that' +
                        ' no longer exist on the filesystem.')
    parser.add_argument('--db', default='check_files.db',
                        help='Database that contains file fingerprint info.')
    parser.add_argument('--dry-run', action='store_true',
                        help='prints commands to execute, but don\'t do ' +
                        'anything.')
    parser.add_argument('root_dir', nargs='+', 
                        help='Root of a directory tree to scan and check.')

    # Create namespace from command-line
    return parser.parse_args()

def setup_logger(quiet_mode, debug_mode, filename=''):
    """Sets up the global logging object.
    """

    # Determine log level
    if debug_mode:
        logLevel = logging.DEBUG
    elif quiet_mode:
        logLevel = logging.WARNING
    else:
        logLevel = logging.INFO

    # Start logger with default level
    logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s', 
                        datefmt='%Y-%m-%d %I:%M:%S', level=logging.INFO,
                        filename=filename)

    # Denote start of logging, if using a file
    if 0 < len(filename):
        logging.info('Logging started')

    # If logging debug messages, indicate so in file.
    if debug_mode:
        logging.info('Logging debug messages is enabled')

    # Set log level
    logging.getLogger().setLevel(logLevel)

def read_in_chunks(file_obj, chunk_size=1024*1024):
    """Generator to read data from the specified file in chunks.
    Default chunk size is 1M.
    """
    while True:
        data = file_obj.read(chunk_size)
        if not data:
            break
        yield data

def calc_fingerprint(filename):
    """Returns a string containing the SHA1 fingerprint of this file.
    """
    # Create a new SHA1 object and read from the specified file in chunks.
    result = hashlib.sha1()
    with open(filename, 'rb') as f:
        for read_data in read_in_chunks(f):
            result.update(read_data)

    # Return fingerprint
    return result.hexdigest()
    
def process_file(node, filename, mode, file_db_dict, cursor):
    """Processes the specified file.
    TO DO
    Returns a tuple that can be used to update stats. 
    Order: (added, good, updated, bad, missing)
    """

    # Concatenate path and filename
    fullname = os.path.join(node[0], filename)

    # Log the file
    logging.info('Processing file \'%s\'', fullname)
    
    # Generate fingerprint
    fp_cpu_time = time.clock()
    fp_real_time = time.time()
    fp = calc_fingerprint(fullname)
    logging.debug('File \'%s\' finished in %.4f seconds (%.4f CPU seconds)', 
                  fullname, time.time() - fp_real_time, 
                  time.clock() - fp_cpu_time)

    # Attempt to access this file's entry in the DB dict. If we throw an
    # exception, we'll need to add the file to the database. If no exception
    # process the file.
    try:
        # Split out entries of file_rec to local variables to avoid confusion
        logging.debug('Entry: %s', file_db_dict[filename])
        db_id = file_db_dict[filename][0]
        db_mtime = file_db_dict[filename][1]
        db_fp = file_db_dict[filename][2]
        db_size = file_db_dict[filename][3]

        # Remove file from db dict to indicate that we've seen it.
        del(file_db_dict[filename])
        logging.debug("Marking file '" + fullname + "' as seen.")

        # Check file against database
        if db_fp != fp:
            # Fingerprints don't match...
            logging.debug('File \'' + fullname + '\' does not match ' +
                          'fingerprint in database')
                    
            # Compare modification times. If this file is newer, 
            # update database. Otherwise issue a warning.
            if float(db_mtime) < mode.st_mtime:
                logging.info('File \'' + fullname + '\' is newer than '
                             + 'database record. Updating...')

                update_file(db_id, fp, mode, cursor)

                # Return tuple indicating update
                return (0,0,1,0,0)
            else:
                logging.warning('File \'' + fullname + '\' does not ' +
                                'match fingerprint in database ' +
                                'and is not newer. File could be damaged!')

                # Return tuple indicating problem
                return (0,0,0,1,0)
        else:
            logging.debug('File \'' + fullname + '\' matches ' +
                          'fingerprint in database')

            # Double-check size to make sure we aren't fooling 
            # ourselves.
            if db_size != mode.st_size:
                logging.warning('File sizes do not match for ' +
                                'file \'' + fullname + 
                                '\'! File could be damaged!')
                # Return tuple indicating problem
                return (0,0,0,1,0)
            else:
                logging.debug('File \'' + fullname + 
                              '\' matches file size in database')
                # Return tuple indicating file ok
                return (0,1,0,0,0)

    except KeyError:
        # File doesn't exist in database - add
        add_file(filename, fp, mode, node[1], cursor)

        # Return tuple indicating addition
        return (1,0,0,0,0)

def get_root_from_db(cursor, rootdir):
    """Searches the database using the specified cursor object for the first
    record in the roots table with the same name. If no record exists, one is 
    added. Returns the tuple: (root dir, root id)
    """

    ret_val = -1

    # Search the database for this root by name. If it doesn't exist, add it.
    cursor.execute("SELECT Path_ID from '" + table_names['dirs'] + 
                   "' WHERE Name=? AND Parent_ID=?", (rootdir,-1))
    row = cursor.fetchone()
    if row == None:
        # Doesn't exist - add
        cursor.execute("INSERT INTO '" + table_names['dirs'] + 
                       "'(Name,Parent_ID) VALUES(?,?)", 
                       (rootdir,-1))
        ret_val = cursor.lastrowid
        logging.debug('Root \'' + rootdir + '\' not in database. Added ' +
                      'with ID = ' + str(ret_val))
    else:
        ret_val = row[0]
        logging.debug('Root \'' + rootdir + '\' found in database with ID = ' +
                      str(ret_val))

    # Return Root_ID for this root
    return ret_val

def get_dir_items_from_db(cursor, parent_id):
    """Searches the database using the specified cursor object for all items
    in the dirs and files tables with the specified Parent_ID.
    
    A dict with the following structure is returned:
    * 'dir_entries' = A dict of dir names => (Path_ID) of all dirs in db with
      the specified parent.
    * 'file_entries' = A dict of filenames => (File_ID, LastModified, FP, Size)
      of all files in db with the specified parent.
    """

    ret_val = { 'dir_entries': dict(), 'file_entries': dict() }

    # Search the database for all directories with the specified parent.
    cursor.execute("SELECT Path_ID, Name from '" + table_names['dirs'] + 
                   "' WHERE Parent_ID=?", (parent_id,))

    for entry in cursor.fetchall():
        dir_id = entry[0]
        dir_name = entry[1]
        logging.debug("Found dir '%s' (ID='%s') with parent '%s'.", 
                      dir_name, dir_id, parent_id)
        ret_val['dir_entries'][dir_name] = (dir_id)

    # Search the database for all files with the specified parent.
    cursor.execute("SELECT File_ID, Name, LastModified, Fingerprint, Size " +
                   "from '" + table_names['files'] + 
                   "' WHERE Parent_ID=?", (parent_id,))

    for entry in cursor.fetchall():
        file_id = entry[0]
        file_name = entry[1]
        file_mtime = entry[2]
        file_fp = entry[3]
        file_size = entry[4]
        logging.debug("Found file '%s' (ID='%s') with parent '%s'.", 
                      file_name, file_id, parent_id)
        ret_val['file_entries'][file_name] = (file_id, file_mtime, file_fp, 
                                              file_size)

    # Return entry list
    return ret_val

def add_file(filename, fp, mode, parent_id, cursor):
    """ Adds the specified file with the specified mode, fingerprint, and
    parent_id to the 'files' table. Returns the new entry's File_ID.
    """
    cursor.execute("INSERT INTO '" + table_names['files'] + 
                   "'(Name, Parent_ID, LastModified, " +
                   "Fingerprint," +
                   " Size) VALUES(?, ?, ?, ?, ?)", 
                   (filename, parent_id, mode.st_mtime, fp, mode.st_size))
    ret_val = cursor.lastrowid
    logging.debug("File '%s' with parent %s' added to database with ID = %s",
                  filename, parent_id, str(ret_val))

    return ret_val

def update_file(file_id, fp, mode, cursor):
    """ Updates the specified file with the specified mode, fingerprint, and
    parent_id to the 'files' table.
    """
    cursor.execute("UPDATE '" + table_names['files'] + 
                   "' SET LastModified=?, Fingerprint=?, Size=? WHERE " +
                   "File_ID=?", (mode.st_mtime, fp, mode.st_size, file_id))

def add_dir(path, parent_id, cursor):
    """Adds the specified path with specified parent_id to the 'dirs' table.
    Returns the new entry's Path_ID.
    """

    cursor.execute("INSERT INTO '" + table_names['dirs'] + 
                   "'(Name, Parent_ID) VALUES(?, ?)", 
                   (path, str(parent_id)))
    ret_val = cursor.lastrowid
    logging.debug('Directory \'' + path + '\' added with ID = ' + str(ret_val))
    return ret_val

def prune_files(path, file_data, cursor):
    """This routine is responsible for iterating over the list of files in the
    database but not on the filesystem and either removing them from the 
    database or simply alerting the user, depending on command-line arguments.
    Returns a tuple to be added to the files stats tuple.
    """
    
    if 0 < len(file_data):
        for item in file_data.keys():
            if cmd_args.prune:
                cursor.execute("DELETE FROM '" + table_names['files'] + 
                               "' WHERE File_ID = ?",
                               (file_data[item][0],))
                logging.info("File '%s' pruned from directory '%s'.", item, 
                             path)
            else:
                logging.warning("File '%s' no longer exists in directory '%s'!",
                                item, path)

    return (0, 0, 0, 0, len(item_data['file_entries']) )

def prune_dirs(path, dir_data, cursor):
    """This routine is responsible for iterating over the list of directories in
    the database but not on the filesystem and either removing them from the 
    database or simply alerting the user, depending on command-line arguments.
    Note that it is necessary to remove subdirectories and files!
    Returns a tuple of tuples that contain the updated stats: (files, dirs)
    """

## TO DO!    
    if 0 < len(item_data['dir_entries']):
        for item in item_data['dir_entries'].keys():
            if cmd_args.prune:
                cursor.execute("DELETE FROM '" + table_names['dirs'] + 
                               "' WHERE Path_ID = ?",
                               (item_data['dir_entries'][item],))
                logging.info("Subdirectory '%s' pruned from directory '%s'.", 
                             item, path)
            else:
                logging.warning("Subdirectory '%s' no longer exists in " +
                                "directory '%s'!", item, path)

    return ( (0, 0, 0, 0, len(item_data['file_entries']) ), 
             (0, 0, 0, 0, len(item_data['dir_entries']) ) )

def add_tuples(t1, t2):
    """Properly vector-adds the specified tuples and returns the resulting
    tuple.
    """
    return map(operator.add, t1, t2)

def process_root(rootdir, db_conn):
    """Crawls the specified directory, processing all files that it finds.
    """
    
    # Get DB cursor object
    cursor = db_conn.cursor()
    
    # Remove any trailing OS separators
    rootdir = rootdir.rstrip(os.sep)
    
    dir_id = get_root_from_db(cursor, rootdir)

    # Push root onto queue to start process
    dir_queue = [(rootdir, dir_id)]

    # Order: (added, good, updated, bad, missing)
    file_stats = (0,0,0,0,0)
    dir_stats = (0,0,0,0,0)

    try:
        # Walk the filesystem
        while 0 < len(dir_queue):
            node = dir_queue.pop()

            # Log the directory that we are processing
            logging.info('Processing directory \'%s\'', node[0])

            # Query the database
            db_cpu_time = time.clock()
            db_real_time = time.time()
            dir_db_data = get_dir_items_from_db(cursor, node[1])
            logging.debug("Dir '%s' DB fetched in %.4f seconds (%.4f CPU " +
                          "seconds)",
                          node[0], time.time() - db_real_time, 
                          time.clock() - db_cpu_time)

            ## Process directory contents
            for entry in os.listdir(node[0]):
                # Generate full path to entry
                entry_full_name = os.path.join(node[0], entry)

                # Grab entry filesystem stats
                try:
                    entry_stat = os.stat(entry_full_name)
                    
                    if stat.S_ISREG(entry_stat.st_mode):
                        # Entry is a regular file.
                        
                        # Process file and update stats
                        file_stats = add_tuples(file_stats, 
                                                process_file(node, entry, 
                                                             entry_stat, 
                                                             dir_db_data[ \
                                    'file_entries'],
                                                             cursor))
                        
                    elif stat.S_ISDIR(entry_stat.st_mode):
                        # Attempt to push directory onto stack using data from 
                        # db. If there is an error, assume that the directory is
                        # new. Add it to the DB then push it onto the stack.
                        try:
                            dir_queue.append( (entry_full_name, 
                                               dir_db_data['dir_entries'] \
                                                   [entry]) )
                            
                            del(dir_db_data['dir_entries'][entry])
                            logging.debug('Marking directory \'' + 
                                          entry_full_name +
                                          '\' as seen.')
                            
                            # Update stats
                            dir_stats = add_tuples(dir_stats, (0,1,0,0,0))
                        except KeyError:
                            # Item not in list. Add then append to queue.
                            dir_id = add_dir(entry, node[1], cursor)
                            dir_queue.append( (entry_full_name, dir_id) )

                            # Update stats.
                            dir_stats = add_tuples(dir_stats, (1,0,0,0,0))

                except OSError as e:
                    logging.warning("OSError({0}): {1}".format(e.errno, 
                                                               e.strerror) +
                                    ' on file \'' + entry_full_name + '\'')

            new_stats = prune_items(node[0], dir_db_data, cursor)
            file_stats = add_tuples(file_stats, new_stats[0])
            dir_stats = add_tuples(dir_stats, new_stats[1])

    except KeyboardInterrupt:
        logging.error("Interrupt detected, aborting crawl and " +
                      "committing all changes.")
    
    # Commit
    db_conn.commit()

    # Print stats!
    action_names = ('added', 'good', 'updated', 'BAD', 'MISSING')
    logging.info('Finished processing root \'' + root + '\': ')
    logging.info('    Files:')
    for idx in range(0,5):
        logging.info('      %s: %s', action_names[idx], file_stats[idx])
    logging.info('    Directories:')
    for idx in range(0,5):
        logging.info('      %s: %s', action_names[idx], dir_stats[idx])

    # Return stats
    return (file_stats, dir_stats)

def open_db(db_url):
    """Function to open the specified SQLite database and return a Connection
    object to it. If the requisite table structure does not exist, it will be
    created.
    """

    # Connect to database
    conn = sqlite3.connect(database=db_url)

    # Look for fingerprints table
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    found = {'files': False, 'dirs': False }
    for table in cursor.fetchall():
        logging.debug("Found table '" + table[0] + "'")
        for fp_table in table_names:
            if (table[0] == table_names[fp_table]):
                logging.debug("Found " + fp_table + " fingerprint table '" + 
                              table[0] + "'.")
                found[fp_table] = True

    # If not found, create
    if not(found['files']):
        cursor.execute("CREATE TABLE '" + table_names['files'] + 
                       "'(File_ID INTEGER PRIMARY KEY AUTOINCREMENT, " +
                       "Name TEXT, Parent_ID INTEGER, " +
                       "LastModified TEXT, Fingerprint TEXT, Size INTEGER)")
        cursor.execute("CREATE INDEX file_parent_idx ON " + table_names['files']
                      + "(Parent_ID)")
        
    if not(found['dirs']):
        cursor.execute("CREATE TABLE '" + table_names['dirs'] + 
                       "'(Path_ID INTEGER PRIMARY KEY AUTOINCREMENT, " +
                       "Name TEXT, Parent_ID INT)")
        cursor.execute("CREATE INDEX dir_parent_idx ON " + table_names['dirs']
                      + "(Parent_ID)")
        
    return conn

########
# Main #
########

# Start timer
cpu_start_time = time.clock()
real_start_time = time.time()

# Parse command-line arguments
cmd_args = parse_args()

# Set up logger
setup_logger(cmd_args.quiet,cmd_args.debug,cmd_args.log)
logging.debug('Command-line arguments: %s', vars(cmd_args))

# Open fingerprint database
with open_db(cmd_args.db) as db_conn:

    # Process the specified directory trees.
    for root in cmd_args.root_dir:
        process_root(root, db_conn)

# Fini!
logging.info('Finished. Total Run Time = %.4f seconds (%.4f CPU seconds)', 
             time.time() - real_start_time, time.clock() - cpu_start_time)
