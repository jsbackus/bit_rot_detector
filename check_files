#!/usr/bin/python

from __future__ import unicode_literals
import logging
import os
import stat
import hashlib
import argparse
import sqlite3
import operator
import time
import io
import sys
import datetime

###########
# Globals #
###########
table_names = { 'files': 'fp_files', 'dirs': 'fp_dirs', 'roots' : 'fp_roots', 
                'tmp_dirs' : 'tmp_dirs' }

#############
# Functions #
#############
def parse_args():
    """Sets up the argparse object and attempts to parse the command-line.
    """
    
    # Create parser object
    parser = argparse.ArgumentParser(description=
                                     'Tool verify file integrity.')
    parser.add_argument('-l', '--log', nargs='?', 
                        default='', help='log to file instead of console')
    parser.add_argument('-q', '--quiet', action='store_true',
                        help='only display warnings and errors')
    parser.add_argument('-d', '--debug', action='store_true',
                        help='display debugging information')
    parser.add_argument('--db', default='check_files.db',
                        help='Database that contains file fingerprint info.')

    subparsers = parser.add_subparsers(title='subcommands', dest='subcommand',
                                       description='valid subcommands',
                                       help='Subcommand Help')
    # Scan subparser
    scan_mode = subparsers.add_parser('scan', 
                                      help='Scans and checks the filesystem')

    scan_mode.add_argument('-p', '--prune', action='store_true',
                           help='Prunes directories and files from database ' +
                           'that no longer exist on the filesystem.')
    scan_mode.add_argument('--dry-run', action='store_true',
                           help='prints commands to execute, but don\'t do ' +
                           'anything.')
    scan_mode.add_argument('-s', '--skip-recent', action='store_true',
                           help='When scanning, skips files in directories that'
                           + ' have been checked recently. See --expr.')
    scan_mode.add_argument('--expr', default=30, type=int,
                           help="Specifies how old 'recent' is, in days, when" +
                           " skipping recent files.")
    scan_mode.add_argument('-r', '--root', default='',
                           help='Specifies the root.')
    scan_mode.add_argument('target', nargs='+', 
                           help='Root of a directory tree to scan and check.')

    # Dupe files subparser
    dupe_files_mode = subparsers.add_parser('dupe_files', 
                                            help='Scans the database for ' +
                                            'duplicate files by fingerprint.')
    dupe_files_mode.add_argument('output', nargs='?', default='',
                                  help='Optional file to dump list of ' +
                                  'duplicates to.')

    # Dupe subtrees subparser
    dupe_trees_mode = subparsers.add_parser('dupe_trees', 
                                             help='Scans the database for ' +
                                             'duplicate directory subtrees.')
    dupe_trees_mode.add_argument('output', nargs='?', default='',
                                  help='Optional file to dump list of ' +
                                  'duplicates to.')

    # Diff Root subparser
    diff_mode = subparsers.add_parser('diffroot', 
                                      help='Scans the database for ' +
                                      'duplicate directory subtrees.')
    diff_mode.add_argument('output', nargs='?', default='',
                           help='Optional file to dump list of ' +
                           'duplicates to.')
    diff_mode.add_argument('root_names', nargs=2, 
                           help='Names of roots to check')

    # Check DB subparser
    checkdb_mode = subparsers.add_parser('checkdb', 
                                         help='Fingerprints the database in ' +
                                         'order to verify it.')

    # list subparser
    list_mode = subparsers.add_parser('list',
                                      help='Displays database contents')
    list_mode.add_argument('target', nargs='*', default='',
                           help='Entry to list. Format: <root>/<path>')

    # delroot subparser
    delroot_mode = subparsers.add_parser('delroot',
                                      help='Removes roots and their contents ' +
                                         'from the database.')
    delroot_mode.add_argument('target', nargs='+', default='',
                           help='Root to remove.')

    # Create namespace from command-line
    return parser.parse_args()

def setup_logger(quiet_mode, debug_mode, filename=''):
    """Sets up the global logging object.
    """

    # Determine log level
    if debug_mode:
        logLevel = logging.DEBUG
    elif quiet_mode:
        logLevel = logging.WARNING
    else:
        logLevel = logging.INFO

    # Start logger with default level
    logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s', 
                        datefmt='%Y-%m-%d %I:%M:%S', level=logging.INFO,
                        filename=filename)

    # Denote start of logging, if using a file
    if 0 < len(filename):
        logging.info('Logging started')

    # If logging debug messages, indicate so in file.
    if debug_mode:
        logging.info('Logging debug messages is enabled')

    # Set log level
    logging.getLogger().setLevel(logLevel)

def tokenize_path(path):
    """Generator to tokenize a path via os.path.split. 
    Note: tokens will return in reverse order!
    """
    tmp_path = path
    while True:
        elms = os.split(tmp_path)
        if len(elms[1]) <= 0:
            break
        tmp_path = elms[0]
        yield elms[1]

def read_in_chunks(file_obj, chunk_size=1024*1024):
    """Generator to read data from the specified file in chunks.
    Default chunk size is 1M.
    """
    while True:
        data = file_obj.read(chunk_size)
        if not data:
            break
        yield data

def calc_fingerprint(filename):
    """Returns a string containing the SHA1 fingerprint of this file.
    """
    # Create a new SHA1 object and read from the specified file in chunks.
    result = hashlib.sha1()
    with open(filename, 'rb') as f:
        for read_data in read_in_chunks(f):
            result.update(read_data)

    # Return fingerprint
    return result.hexdigest()
    
def process_file(node, filename, mode, file_db_dict, cursor):
    """Processes the specified file.
    TO DO
    Returns a tuple that can be used to update stats. 
    Order: (added, good, updated, bad, missing)
    """

    # Concatenate path and filename
    fullname = os.path.join(node[0], filename)

    # Log the file
    logging.info('Processing file \'%s\'', fullname)
    
    # Generate fingerprint
    fp_cpu_time = time.clock()
    fp_real_time = time.time()
    fp = calc_fingerprint(fullname)
    logging.debug('File \'%s\' finished in %.4f seconds (%.4f CPU seconds)', 
                  fullname, time.time() - fp_real_time, 
                  time.clock() - fp_cpu_time)

    # Attempt to access this file's entry in the DB dict. If we throw an
    # exception, we'll need to add the file to the database. If no exception
    # process the file.
    try:
        # Split out entries of file_rec to local variables to avoid confusion
        logging.debug('Entry: %s', file_db_dict[filename])
        db_id = file_db_dict[filename][0]
        db_mtime = file_db_dict[filename][1]
        db_fp = file_db_dict[filename][2]
        db_size = file_db_dict[filename][3]

        # Remove file from db dict to indicate that we've seen it.
        del(file_db_dict[filename])
        logging.debug("Marking file '" + fullname + "' as seen.")

        # Check file against database
        if db_fp != fp:
            # Fingerprints don't match...
            logging.debug('File \'' + fullname + '\' does not match ' +
                          'fingerprint in database')
                    
            # Compare modification times. If this file is newer, 
            # update database. Otherwise issue a warning.
            if float(db_mtime) < mode.st_mtime:
                logging.info('File \'' + fullname + '\' is newer than '
                             + 'database record. Updating...')

                update_file(db_id, fp, mode, cursor)

                # Return tuple indicating update
                return (0,0,1,0,0,0)
            else:
                logging.warning('File \'' + fullname + '\' does not ' +
                                'match fingerprint in database ' +
                                'and is not newer. File could be damaged!')

                # Return tuple indicating problem
                return (0,0,0,1,0,0)
        else:
            logging.debug('File \'' + fullname + '\' matches ' +
                          'fingerprint in database')

            # Double-check size to make sure we aren't fooling 
            # ourselves.
            if db_size != mode.st_size:
                logging.warning('File sizes do not match for ' +
                                'file \'' + fullname + 
                                '\'! File could be damaged!')
                # Return tuple indicating problem
                return (0,0,0,1,0,0)
            else:
                logging.debug('File \'' + fullname + 
                              '\' matches file size in database')
                # Return tuple indicating file ok
                return (0,1,0,0,0,0)

    except KeyError:
        # File doesn't exist in database - add
        add_file(filename, fp, mode, node[1], cursor)

        # Return tuple indicating addition
        return (1,0,0,0,0,0)

def get_root_from_db(cursor, rootdir):
    """Searches the database using the specified cursor object for the first
    record in the roots table with the same name. If no record exists, one is 
    added. Returns the tuple: (root dir, root id, last_checked)
    """

    ret_val = -1

    # Search the database for this root by name. If it doesn't exist, add it.
    cursor.execute("SELECT Path_ID, LastChecked from '" + table_names['dirs'] + 
                   "' WHERE Name=? AND Parent_ID=?", (rootdir,-1))
    row = cursor.fetchone()
    if row == None:
        # Doesn't exist - add
        cursor.execute("INSERT INTO '" + table_names['dirs'] + 
                       "'(Name,Parent_ID) VALUES(?,?)", 
                       (rootdir,-1))
        ret_val = (cursor.lastrowid, "")
        logging.debug('Root \'' + rootdir + '\' not in database. Added ' +
                      'with ID = ' + str(ret_val))
    else:
        ret_val = (row[0], row[1])
        logging.debug('Root \'' + rootdir + '\' found in database with ID = ' +
                      str(ret_val))

    # Return Root_ID for this root
    return ret_val

def get_dir_items_from_db(cursor, parent_id, check_files):
    """Searches the database using the specified cursor object for all items
    in the dirs and files tables with the specified Parent_ID.
    
    A dict with the following structure is returned:
    * 'dir_entries' = A dict of dir names => (Path_ID) of all dirs in db with
      the specified parent.
    * 'file_entries' = A dict of filenames => (File_ID, LastModified, FP, Size)
      of all files in db with the specified parent.
    """

    ret_val = { 'dir_entries': dict(), 'file_entries': dict() }

    # Search the database for all directories with the specified parent.
    cursor.execute("SELECT Path_ID, Name, LastChecked from '" + 
                   table_names['dirs'] + "' WHERE Parent_ID=?", (parent_id,))

    for entry in cursor.fetchall():
        dir_id = entry[0]
        dir_name = entry[1]
        last_checked = entry[2]
        logging.debug("Found dir '%s' (ID='%s') with parent '%s'.", 
                      dir_name, dir_id, parent_id)
        ret_val['dir_entries'][dir_name] = (dir_id, last_checked)

    # Search the database for all files with the specified parent, unless
    # we're skipping the files in this directory.
    if check_files:
        cursor.execute("SELECT File_ID, Name, LastModified, Fingerprint, Size" +
                       " from '" + table_names['files'] + 
                       "' WHERE Parent_ID=?", (parent_id,))

        for entry in cursor.fetchall():
            file_id = entry[0]
            file_name = entry[1]
            file_mtime = entry[2]
            file_fp = entry[3]
            file_size = entry[4]
            logging.debug("Found file '%s' (ID='%s') with parent '%s'.", 
                          file_name, file_id, parent_id)
            ret_val['file_entries'][file_name] = (file_id, file_mtime, file_fp, 
                                                  file_size)

    # Return entry list
    return ret_val

def add_file(filename, fp, mode, parent_id, cursor):
    """ Adds the specified file with the specified mode, fingerprint, and
    parent_id to the 'files' table. Returns the new entry's File_ID.
    """
    cursor.execute("INSERT INTO '" + table_names['files'] + 
                   "'(Name, Parent_ID, LastModified, " +
                   "Fingerprint," +
                   " Size) VALUES(?, ?, ?, ?, ?)", 
                   (filename, parent_id, mode.st_mtime, fp, mode.st_size))
    ret_val = cursor.lastrowid
    logging.debug("File '%s' with parent %s' added to database with ID = %s",
                  filename, parent_id, str(ret_val))

    return ret_val

def update_file(file_id, fp, mode, cursor):
    """ Updates the specified file with the specified mode, fingerprint, and
    parent_id to the 'files' table.
    """
    cursor.execute("UPDATE '" + table_names['files'] + 
                   "' SET LastModified=?, Fingerprint=?, Size=? WHERE " +
                   "File_ID=?", (mode.st_mtime, fp, mode.st_size, file_id))

def add_dir(path, parent_id, cursor):
    """Adds the specified path with specified parent_id to the 'dirs' table.
    Returns the new entry's Path_ID.
    """

    cursor.execute("INSERT INTO '" + table_names['dirs'] + 
                   "'(Name, Parent_ID) VALUES(?, ?)", 
                   (path, str(parent_id)))
    ret_val = (cursor.lastrowid, None)
    logging.debug('Directory \'' + path + '\' added with ID = ' + str(ret_val))
    return ret_val

def mark_dir_checked(dir_id, cursor):
    """Marks the specified directory as "recently seen".
    """

    tmp_now = datetime.datetime.now()
    cursor.execute("UPDATE '" + table_names['dirs'] + 
                   "' SET LastChecked=? WHERE Path_ID=?", (tmp_now, dir_id))
                                                           

def prune_files(path, file_data, cursor):
    """This routine is responsible for iterating over the list of files in the
    database but not on the filesystem and either removing them from the 
    database or simply alerting the user, depending on command-line arguments.
    Returns a tuple to be added to the files stats tuple.
    """
    
    if 0 < len(file_data):
        for item in file_data.keys():
            if cmd_args.prune:
                cursor.execute("DELETE FROM '" + table_names['files'] + 
                               "' WHERE File_ID = ?",
                               (file_data[item][0],))
                logging.info("File '%s' pruned from directory '%s'.", item, 
                             path)
            else:
                logging.warning("File '%s' no longer exists in directory '%s'!",
                                item, path)

    return (0, 0, 0, 0, len(file_data), 0 )

def prune_dirs(path, dir_data, cursor):
    """This routine is responsible for iterating over the list of directories in
    the database but not on the filesystem and either removing them from the 
    database or simply alerting the user, depending on command-line arguments.
    Note that it is necessary to remove subdirectories and files!
    Returns a tuple of tuples that contain the updated stats: (files, dirs)
    """

    done = False
    ret_val = [ (0, 0, 0, 0, 0, 0), (0, 0, 0, 0, 0, 0) ]
    item_data = { 'file_entries': dict(), 'dir_entries': dir_data }

    # Build a queue like in crawl_tree of all directories to be pruned.
    dir_queue = []

    while not done:
        # Each round, append any directories in the database dict to the queue.
        for item in item_data['dir_entries'].keys():
            dir_queue.append( ( path, item, item_data['dir_entries'][item]) )
            
        # Prune any files
        ret_val[0] = add_tuples(ret_val[0], prune_files(path, item_data[ \
                    'file_entries'], cursor))
        
        if 0 < len(dir_queue):
            # Pop the next directory off of the stack
            node = dir_queue.pop()

            # Update path to refer to this item
            path = os.path.join(node[0], node[1])

            # Grab all entries for this directory from the database
            item_data = get_dir_items_from_db(cursor, node[2])

            # Delete this item from the database, if appropriate
            if cmd_args.prune:
                cursor.execute("DELETE FROM '" + table_names['dirs'] + 
                               "' WHERE Path_ID = ?",
                               (node[2],))
                logging.info("Subdirectory '%s' pruned from directory '%s'.", 
                             node[1], node[0])
            else:
                logging.warning("Subdirectory '%s' no longer exists in " +
                                "directory '%s'!", node[1], node[0])

            ret_val[1] = add_tuples(ret_val[1], (0, 0, 0, 0, 1, 0))
        else:
            done = True

    return ret_val

def add_tuples(t1, t2):
    """Properly vector-adds the specified tuples and returns the resulting
    tuple.
    """
    return map(operator.add, t1, t2)

def resolve_targets(cursor, targets)

    # this is temporary...
    roots = []
    
    targ_files = []
    targ_dirs = []
    for trgt in targets:
        trgt_path = trgt.rstrip(os.sep)
        if os.path.isreg(trgt[1]):
            # target is a file. Name = basename
            targ_files.append( ( os.path.basename(trgt_path), trgt_path ) )
        elif os.path.isdir(trgt[1]):
            # target is a directory. Name = target, unless it is "." or "..",
            # in which case convert to basename of absolute path.
            if trgt_path == "." or trgt_path == "..":
                trgt_name = os.path.basename( os.path.abspath(trgt_path) )
            else:
                trgt_name = trgt
            targ_dirs.append( ( trgt_name, trgt_path ) )
        else:
            logging.error("Invalid target specified: '%s'", trgt[1])

    # If a root wasn't specified with --root, try the simple cases. Applies to
    # directories only!
    if len(cmd_args.root) <= 0:
        # Check to see if anything is in the roots table. If not, assume each
        # target is actually a root.
        cursor.execute("SELECT COUNT(*) FROM '" + table_names['roots'] + "'")
        if cursor.fetchone() <= 0:
            logging.debug("Roots table is empty. Assuming targets are roots: " +
                          "%s", targ_dirs)
            for trgt in targ_dirs:
                roots.append( trgt )
            targ_dirs = []
        # Search roots table for a target with the same name. Move each match
        # to the roots list and remove from the targets list.
        for trgt in targ_dirs:
            cursor.execute( "SELECT COUNT(*) FROM '" + table_names['roots']
                            + "' WHERE Name=?", (trgt[0],) )
            if 0 < cursor.fetchone():
                logging.debug("Target %s matches a root name and no root " +
                              "was specified.", trgt[0])
                roots.append( trgt )
                targ_dirs.remove( trgt )

    # build a list of file entries that match entries in targ_files.
    if (0 < len(targ_files)) or (0 < len(targ_dirs)):
        cursor.execute("CREATE TEMP TABLE TMP_VALID_FILES (File_ID INT,"+
                       "Name TEXT,Parent_ID INT)")
        cursor.execute("CREATE TEMP TABLE TMP_VALID_DIRS (Path_ID INT,"+
                       "Name TEXT,Parent_ID INT")
        if 0 < len(targ_files) :
            cursor.execute("CREATE TEMP TABLE TMP_TARG_FILES (Name TEXT)")
            for trgt in targ_files:
                cursor.execute( "INSERT INTO TMP_TARG_FILES (Name) VALUES(?)", 
                                (trgt[0],) )
                cursor.execute("INSERT INTO TMP_VALID_FILES SELECT File_ID," +
                               "Name,Parent_ID FROM '" + table_names['files'] +
                               "' WHERE Name IN (SELECT Name FROM "+
                               "TMP_TARG_FILES)")
        
        if 0 < len(targ_dirs) :
            cursor.execute("CREATE TEMP TABLE TMP_TARG_DIRS (Name TEXT)")
            for trgt in targ_dirs:
                cursor.execute( "INSERT INTO TMP_TARG_DIRS (Name) VALUES(?)", 
                            (trgt[0],) )
                cursor.execute("INSERT INTO TMP_VALID_DIRS Path_ID,Name," +
                               "Parent_ID FROM '" + table_names['files'] +
                               "' WHERE Name IN (SELECT Name FROM "
                               +"TMP_TARG_DIRS)")
        # Reconstruct applicable tree
        create_tmp_dir_table( cursor )
        cursor.execute( "INSERT INTO '" + table_names['tmp_dirs'] + 
                        "' (Path_ID,Parent_ID) SELECT Path_ID,Parent_ID FROM '"
                        + table_names['dirs'] + "' WHERE Path_ID IN (SELECT " +
                        "Parent_ID FROM TMP_VALID_FILES)")
        cursor.execute( "INSERT INTO '" + table_names['tmp_dirs'] + 
                        "' (Path_ID,Parent_ID) SELECT Path_ID,Parent_ID FROM '"
                        + table_names['dirs'] + "' WHERE Path_ID IN (SELECT " +
                        "Path_ID FROM TMP_VALID_DIRS)")
        cursor.execute( "INSERT INTO '" + table_names['tmp_dirs'] + 
                        "' (Path_ID,Parent_ID) SELECT Path_ID,Parent_ID FROM '"
                        + table_names['dirs'] + "' WHERE Path_ID IN (SELECT " +
                        "Parent_ID FROM TMP_VALID_DIRS)")
        tree_info = reconstruct_tree(cursor)
        drop_tmp_dir_table( cursor )

        # Retrieve list of valid files and stuff into a dict of lists:
        # name : (file_id,parent_id)
        cursor.execute( "SELECT Name,File_ID,Parent_ID FROM TMP_VALID_FILES" )
        valid_files = dict()
        for row in cursor.fetchall():
            if not valid_files.has_key( row[0] ):
                valid_files[ row[0] ] = []
            valid_files[ row[0] ].append( ( row[1], row[2] ) )

        # Retrieve list of valid dirs and stuff into a dict of lists:
        # name : (path_id,parent_id)
        cursor.execute( "SELECT Name,Path_ID,Parent_ID FROM TMP_VALID_DIRS" )
        valid_dirs = dict()
        for row in cursor.fetchall():
            if not valid_dirs.has_key( row[0] ):
                valid_dirs[ row[0] ] = []
            valid_dirs[ row[0] ].append( ( row[1], row[2] ) )

        # Step through list of target files, removing any valid ones
#        for trgt in targ_files:
#            if valid_files.has_key(trgt):
#                targ_files.remove(trgt)

        # Step through list of target directories seeing if we have a match.
        # If so, add to roots and remove from targ_dirs.
        for trgt in targ_dirs:
            if valid_dirs.has_key(trgt):
                for entry in valid_dirs[trgt]:
                    

                targ_dirs.remove(trgt)
            
    
    # build a list of dir entries that match base names in targ_dirs

    # For any files and any remaining directories, tokenize the paths and
    # attempt to resolve into root/subtree.
    targ_parts = dict()
    if 0 < len(targ_dirs):
        for trgt in targ_dirs:
            for token in tokenize_path( os.path.abspath( trgt[1] ) ):
                targ_parts[ token ] = 1
    if 0 < len(targ_files):
        for trgt in targ_files:
            for token in tokenize_path( os.path.dirname( 
                    os.path.abspath( trgt[1] ) ) ):
                targ_parts[ token ] = 1

    if 0 < len(targ_parts):
        # Now stuff the tokens into a temporary table and select all matching
        # entries from the directory table
        cursor.execute("CREATE TEMP TABLE TMP_TARG_DIRS (Name TEXT)")
        for token in targ_parts.keys():
            cursor.execute( "INSERT INTO TMP_TARG_DIRS (Name) VALUES(?)", 
                            (token,) )
        
        create_tmp_dir_table( cursor )
        cursor.execute( "INSERT INTO '" + table_names['tmp_dirs'] + 
                        "' (Path_ID,Parent_ID) " +
                        "SELECT Path_ID,Parent_ID FROM '" + table_names['dirs'] 
                        + "' WHERE Name IN (SELECT Name FROM TMP_TARG_DIRS " +
                        "GROUP BY Parent_ID)")
        tree_info = reconstruct_tree(cursor)

# TO DO

    return { 'files' : targ_files, 'dirs' : targ_dirs }

def scan_targets(db_conn):
    """Attempts to determine the root associated with the each target, then
    scans the target, checking all files and directories that belong to it.
    Returns 0 if successful or 1 if error.
    """

    # Get DB cursor object
    cursor = db_conn.cursor()

    targets = resolve_targets(cursor, cmd_args.target)

## TO DO

    # Process any file targets
    for target in targets['files']:
        pass

    # Now process roots
    for target in targets['dirs']:
## TO DO
        root_info = get_root_from_db(cursor, target)
        if( 0 < crawl_tree(rootdir, root_info[0], root_info[1], cursor) ):
            break

def crawl_tree(rootdir, dir_id, last_checked, cursor):
    """Crawls the specified directory, processing all files that it finds.
    Returns 0 if successful or 1 if error.
    """

    # Build timedelta object used to determine if entities have expired.
    expr_check = datetime.timedelta(cmd_args.expr)

    # Push root onto queue to start process
    dir_queue = [(rootdir, dir_id, last_checked)]

    # Order: (added, good, updated, bad, missing, skipped)
    file_stats = (0,0,0,0,0,0)
    dir_stats = (0,0,0,0,0,0)

    error_flag = 0

    try:
        # Walk the filesystem
        while 0 < len(dir_queue):
            node = dir_queue.pop()

            # Log the directory that we are processing
            logging.info("Processing directory '%s'", node[0])
            logging.debug("%s", node)

            # Figure out if we're skipping the files in this directory
            check_files = True
            if cmd_args.skip_recent:
                if (node[2] != None) and (0 < len(str(node[2]))):
                    tmp_delta = datetime.datetime.now() - node[2]
                    if not (expr_check < tmp_delta):
                        check_files = False
                        logging.info("Skipping files in '%s' because it was " +
                                     "last checked %s days ago.", node[0], 
                                     tmp_delta.days)

            # Query the database
            db_cpu_time = time.clock()
            db_real_time = time.time()
            dir_db_data = get_dir_items_from_db(cursor, node[1], check_files)
            logging.debug("Dir '%s' DB fetched in %.4f seconds (%.4f CPU " +
                          "seconds)",
                          node[0], time.time() - db_real_time, 
                          time.clock() - db_cpu_time)

            ## Process directory contents
            for entry in os.listdir(node[0]):
                # Generate full path to entry
                entry_full_name = os.path.join(node[0], entry)

                # Grab entry filesystem stats
                try:
                    entry_stat = os.stat(entry_full_name)

                    if os.path.islnk(entry_full_name):
                        # Entry is a symbolic link, which we ignore.
                        logging.info("Skipping symbolic link '%s'",
                                     entry_full_name)
                        if stat.S_ISREG(entry_stat.st_mode):
                            file_stats = add_tuples(file_stats, 
                                                    (0, 0, 0, 0, 0, 1))
                        elif stat.S_ISDIR(entry_stat.st_mode):
                            dir_stats = add_tuples(dir_stats, 
                                                    (0, 0, 0, 0, 0, 1))
                    
                    elif stat.S_ISREG(entry_stat.st_mode):
                        # Entry is a regular file.
                        
                        # Process file and update stats, unless we're skipping
                        # files in this directory
                        if check_files:
                            file_stats = add_tuples(file_stats, 
                                                    process_file(node, entry, 
                                                                 entry_stat, 
                                                                 dir_db_data[ \
                                        'file_entries'],
                                                                 cursor))
                        else:
                            file_stats = add_tuples(file_stats, 
                                                    (0, 0, 0, 0, 0, 1))
                        
                    elif stat.S_ISDIR(entry_stat.st_mode):
                        # Attempt to push directory onto stack using data from 
                        # db. If there is an error, assume that the directory is
                        # new. Add it to the DB then push it onto the stack.
                        try:
                            tmp_data = dir_db_data['dir_entries'][entry]
                            dir_queue.append( (entry_full_name, tmp_data[0],
                                               tmp_data[1]) )
                            
                            del(dir_db_data['dir_entries'][entry])
                            logging.debug('Marking directory \'' + 
                                          entry_full_name +
                                          '\' as seen.')
                            
                            # Update stats
                            dir_stats = add_tuples(dir_stats, (0,1,0,0,0,0))
                        except KeyError:
                            # Item not in list. Add then append to queue.
                            tmp_data = add_dir(entry, node[1], cursor)
                            dir_queue.append( (entry_full_name, tmp_data[0],
                                               tmp_data[1]) )

                            # Update stats.
                            dir_stats = add_tuples(dir_stats, (1,0,0,0,0,0))

                except OSError as e:
                    logging.warning("OSError({0}): {1}".format(e.errno, 
                                                               e.strerror) +
                                    " on file '" + entry_full_name + "'")

            file_stats = add_tuples(file_stats, prune_files(node[0], 
                                                            dir_db_data[ \
                        'file_entries'], cursor))
            tmp_stats = prune_dirs(node[0], dir_db_data['dir_entries'], cursor)
            file_stats = add_tuples(file_stats, tmp_stats[0])
            dir_stats = add_tuples(dir_stats, tmp_stats[1])

            # Mark this directory has recently checked, if appropriate
            if check_files:
                mark_dir_checked(node[1], cursor)

    except KeyboardInterrupt:
        error_flag = 1
        logging.error("Interrupt detected, aborting crawl and " +
                      "committing all changes.")
    
    # Commit
    db_conn.commit()

    # Print stats!
    action_names = ('added', 'good', 'updated', 'BAD', 'MISSING', 'Skipped')
    logging.info('Finished processing root \'' + root + '\': ')
    logging.info('    Files:')
    for idx in range(0,len(action_names)):
        logging.info('      %s: %s', action_names[idx], file_stats[idx])
    logging.info('    Directories:')
    for idx in range(0,len(action_names)):
        logging.info('      %s: %s', action_names[idx], dir_stats[idx])

    # Return error code
    return error_flag

def open_db(db_url):
    """Function to open the specified SQLite database and return a Connection
    object to it. If the requisite table structure does not exist, it will be
    created.
    """

    # Connect to database
    conn = sqlite3.connect(database=db_url, 
                           detect_types=sqlite3.PARSE_DECLTYPES)

    # Look for fingerprints table
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    found = {'files': False, 'dirs': False, 'roots': False }
    for table in cursor.fetchall():
        logging.debug("Found table '" + table[0] + "'")
        for fp_table in table_names:
            if (table[0] == table_names[fp_table]):
                logging.debug("Found " + fp_table + " fingerprint table '" + 
                              table[0] + "'.")
                found[fp_table] = True

    # If not found, create
    if not(found['files']):
        cursor.execute("CREATE TABLE '" + table_names['files'] + 
                       "'(File_ID INTEGER PRIMARY KEY AUTOINCREMENT, " +
                       "Name TEXT, Parent_ID INTEGER, " +
                       "LastModified TEXT, Fingerprint TEXT, Size INTEGER)")
        cursor.execute("CREATE INDEX file_parent_idx ON " + table_names['files']
                      + "(Parent_ID)")
        
    if not(found['dirs']):
        cursor.execute("CREATE TABLE '" + table_names['dirs'] + 
                       "'(Path_ID INTEGER PRIMARY KEY AUTOINCREMENT, " +
                       "Name TEXT, Parent_ID INT, LastChecked TIMESTAMP)")
        cursor.execute("CREATE INDEX dir_parent_idx ON " + table_names['dirs']
                      + "(Parent_ID)")
        
    if not(found['roots']):
        cursor.execute("CREATE TABLE '" + table_names['roots'] + 
                       "'(Alias_ID INTEGER PRIMARY KEY AUTOINCREMENT, " +
                       "Name TEXT, Path, Path_ID INT)")
        
    return conn

def reconstruct_tree(cursor):
    """Iteratively fetches ancestores for all items in the tmp_dirs table until
    all ancestors have been retrieved. Then reconstructs the paths for each 
    entry, returning a dict of dicts: 
    * roots : <path_id> : <root_name>
    * dirs : <path_id> : (<full path sans root name>, root id)
    """
    
    ret_val = { "roots" : dict(), "dirs" : dict() }
    done = False

    while not done:
        if ( 0 < cursor.rowcount ):
            cursor.execute("INSERT INTO '" + table_names['tmp_dirs'] + 
                           "' (Path_ID,Parent_ID" +
                           ") SELECT Path_ID,Parent_ID FROM '" + 
                           table_names['dirs'] + "' WHERE Path_ID IN " +
                           "(SELECT Parent_ID FROM '" + 
                           table_names['tmp_dirs'] + "' WHERE Parent_ID NOT " +
                           "IN (SELECT Path_ID FROM '" + 
                           table_names['tmp_dirs'] + "'))")
        else:
            done = True

    # Now retrieve directory names and place into a hash of path_id -> name
    # so that we can rebuild path names.
    # Note: we're expecting parents to have a numerically smaller ID than
    # their children. This will hold true as long as parents are always 
    # added to the database before processing children.
    parent_map = dict()
    cursor.execute("SELECT Path_ID,Parent_ID,Name FROM '" + 
                   table_names['dirs'] +
                   "' WHERE Path_ID IN (SELECT Path_ID FROM '" +
                   table_names['tmp_dirs'] + "') ORDER BY Path_ID")
    
    dir_node_count = 0
    for entry in cursor.fetchall():
        if entry[1] < 0:
            # Root, so wrap it in brackets
            ret_val['roots'][ entry[0] ] = entry[2]
        else:
            # Check to see if this directory's parent is in the list.
            # If not, parent is a root, so check there.
            if(ret_val['dirs'].has_key( entry[1] )):
                parent_node = ret_val['dirs'][ entry[1] ]
                ret_val['dirs'][ entry[0] ] = (os.path.join( \
                        parent_node[0], entry[2] ), parent_node[1])
                dir_node_count += 1
            elif(ret_val['roots'].has_key( entry[1] )): 
                ret_val['dirs'][ entry[0] ] = (os.path.join( "", entry[2] ), 
                                               entry[1] )
                dir_node_count += 1
            else:
                logging.error("Parent %s for directory %s (%s) not in map!",
                              entry[1], entry[2], entry[0])
                                                  
            
    logging.info("Directory nodes processed: " + str(dir_node_count))
    
    return ret_val

def gen_db_url(tree_info, parent_id, item):
    """Generates a suitable string representation of the specified entry in the
    database using the reconstructed tree information from reconstruct_tree().
    """

    if tree_info['dirs'].has_key( parent_id ):
        root_id = tree_info['dirs'][ parent_id ][1]
        ret_val = '[' + tree_info['roots'][ root_id ] + ']'
        ret_val = os.path.join(ret_val, tree_info['dirs'][ parent_id ][0], 
                               item)
    else:
        ret_val = '[' + tree_info['roots'][ parent_id ] + ']'
        ret_val = os.path.join(ret_val, item)
        
    return ret_val

def create_tmp_dir_table(cursor):
    """Creates the temporary directory table
    """
    cursor.execute("CREATE TEMP TABLE '" + table_names['tmp_dirs'] + 
                   "' (Path_ID INTEGER " +
                   "PRIMARY KEY, Parent_ID INTEGER)")

def drop_tmp_dir_table(cursor):
    """Deletes the temporary dir table
    """
    cursor.execute("DROP TABLE '" + table_names['tmp_dirs'] + "'")

def check_duplicates(db_conn):
    """Scans the database looking for duplicate files by fingerprint. Prints
    a list of files to STDOUT.
    """

    # Dict of lists by fingerprint
    file_hash = dict()
    
    # Dict of directories by ID
    dir_hash = dict()

    # Count of duplicates
    count = 0
    
    # Get DB cursor object
    cursor = db_conn.cursor()

    # Create a temporary table containing all duplicate files
    cursor.execute("CREATE TEMP TABLE TMP_DUPES AS SELECT " +
                   "File_ID,Name,Fingerprint,Parent_ID FROM '" + 
                   table_names['files'] + "' WHERE " +
                   "Fingerprint IN (SELECT Fingerprint from '" + 
                   table_names['files'] + "' GROUP BY Fingerprint HAVING " +
                   "1<COUNT(*)) ORDER BY Fingerprint")

    # Select all duplicate files
    cursor.execute("SELECT * FROM TMP_DUPES")
    for entry in cursor.fetchall():
        count += 1
        try:
            # Attempt to append the current entry to the end of list for 
            # its fingerprint
            file_hash[entry[2]].append( (entry[0], entry[1], entry[3]) )
        except KeyError:
            # New entry
            file_hash[entry[2]] = [ (entry[0], entry[1], entry[3]) ]

    # Populate directory lookup table so that we can reconstruct each path.
    # We'll use a SQL to do the heavy lifting to avoid shipping data back and
    # forth.
    if 0 < count:
        # Create temp table
        create_tmp_dir_table(cursor)

        # First pass, direct parents from files temp table
        cursor.execute("INSERT INTO '" + table_names['tmp_dirs'] + 
                       "' (Path_ID,Parent_ID) " +
                       "SELECT Path_ID,Parent_ID FROM '" + table_names['dirs'] +
                       "' WHERE Path_ID IN (SELECT Parent_ID FROM TMP_DUPES " +
                       "GROUP BY Parent_ID)")

        # Now reconstruct tree
        tree_info = reconstruct_tree(cursor)

        ## Display results

        # Open dupes file, if specified
        fh = None
        if 0 < len(cmd_args.output):
            fh = io.open(cmd_args.dupes_file, 'wt')
        elif cmd_args.quiet or (0 < len(cmd_args.log)):
            fh = io.open(sys.stdout.fileno(), 'wt')
        
        for fp in file_hash.keys():
            # Send to log
            logging.info(str(len(file_hash[fp])) + " files with Fingerprint 0x" 
                         + fp + ":")

            for entry in file_hash[fp]:
                path_name = gen_db_url( tree_info, entry[2], entry[1] )
                logging.info("    " + path_name + " (ID = " + str(entry[0]) + 
                             ")")

            # Send to file/STDOUT if appropriate
            if not (fh == None):
                fh.write(str(len(file_hash[fp])) + " files with Fingerprint 0x" 
                         + fp + ":" + os.linesep)

                for entry in file_hash[fp]:
                    path_name = gen_db_url( tree_info, entry[2], entry[1] )
                    logging.info("    " + path_name + " (ID = " + str(entry[0]) 
                             + ")")
                    fh.write("    " + path_name + os.linesep)

        if not (fh == None):
            fh.close()

    drop_tmp_dir_table(cursor)
    logging.info("Duplicates found: " + str(count))

def write_db_fp(sha1, filename):
    """Helper function to write the specified sha1 to the specified filename.
    """
    try:
        fh = io.open(filename, 'wt')
        payload = sha1 + os.linesep
        fh.write(unicode(payload))
        fh.close()
    except IOError as e:
        logging.error("IOError({0}): {1}".format(e.errno, e.strerror) +
                      " on file '" + filename + "'")


def check_db():
    """Calculates the fingerprint of the database and checks it against the
    fingerprint in a file with the same name + 'sha1' extension. In the event
    that the fingerprints do not match, the mod times of the two files are
    compared. If the db was modified after the fingerprint file, the database
    is assumed fine and the fingerprint file is updated.
    """

    # For convenience...
    fullname = cmd_args.db

    # Log the file
    logging.info("Fingerprinting database '%s'", fullname)
    
    # Generate fingerprint
    fp_cpu_time = time.clock()
    fp_real_time = time.time()
    fp = calc_fingerprint(fullname)
    logging.debug("Database '%s' has fingerprint '0x%s'", fullname, fp)
    logging.debug("File '%s' finished in %.4f seconds (%.4f CPU seconds)", 
                  fullname, time.time() - fp_real_time, 
                  time.clock() - fp_cpu_time)

    # Generate fingerprint filename
    fp_file = fullname + ".sha1"

    # Check against fingerprint file
    try:
        # Attempt to open the file. If we can't, assume that we need to create
        # a new one.
        fh = io.open(fp_file, 'rt')

        last_fp = fh.readline().rstrip()
        logging.debug("Previous fingerprint = '0x%s'", last_fp)

        if fp != last_fp:
            logging.debug("Database fingerprint does not match previous " +
                          "fingerprint.")

            # Check modification times.
            db_stat = os.stat(fullname)
            fp_stat = os.stat(fp_file)
            if fp_stat.st_mtime < db_stat.st_mtime:
                # Database has been touched since last fingerprint. Update.
                logging.info("Database '" + fullname + "' newer than " +
                             "fingerprint file '" + fp_file + "'. Updating...")
                write_db_fp(fp, fp_file)
            else:
                # Looks like something squirrelly is going on. Alert user.
                logging.warning("Database '" + fullname + "' does not match " +
                                "fingerprint file '" + fp_file + 
                                "'. Database could be damaged!")

        else:
            logging.info("Database fingerprint matches previous fingerprint.")

    except IOError:
        logging.info("Unable to open fingerprint file '" + fp_file + 
                     "'. Generating a new one.")
        write_db_fp(fp, fp_file)

########
# Main #
########

# Start timer
cpu_start_time = time.clock()
real_start_time = time.time()

# Parse command-line arguments
cmd_args = parse_args()

# Set up logger
setup_logger(cmd_args.quiet,cmd_args.debug,cmd_args.log)
logging.debug('Command-line arguments: %s', vars(cmd_args))

try:
    if cmd_args.subcommand == 'scan':
        # Open fingerprint database
        with open_db(cmd_args.db) as db_conn:
            scan_targets(db_conn)
                
    elif cmd_args.subcommand == 'dupe_files':
        # Open fingerprint database
        with open_db(cmd_args.db) as db_conn:
            check_duplicates(db_conn)
            
    elif cmd_args.subcommand == 'dupe_trees':
        print "\n\nWarning: Not supported yet!\n"
        # Open fingerprint database
        #    with open_db(cmd_args.db) as db_conn:
        #        check_duplicates(db_conn)

    elif cmd_args.subcommand == 'diffroot':
        print "\n\nWarning: Not supported yet!\n"
        # Open fingerprint database
        #    with open_db(cmd_args.db) as db_conn:
        #        check_duplicates(db_conn)
        
    elif cmd_args.subcommand == 'list':
        print "\n\nWarning: Not supported yet!\n"
        # Open fingerprint database
#        with open_db(cmd_args.db) as db_conn:
#            db_browse(db_conn)
            
    if cmd_args.subcommand == 'delroot':
        print "\n\nWarning: Not supported yet!\n"
        # Open fingerprint database
#        with open_db(cmd_args.db) as db_conn:
#            # Remove each specified root.
#            for root in cmd_args.root_dir:
#                if (del_root(root, db_conn) != 0):
#                    break

    elif cmd_args.subcommand == 'checkdb':
        check_db()

except KeyboardInterrupt:
    # Catch here also, in case it was missed earlier.
    logging.error("Interrupt detected! Aborting")
    pass

# Fini!
logging.info('Finished. Total Run Time = %.4f seconds (%.4f CPU seconds)', 
             time.time() - real_start_time, time.clock() - cpu_start_time)
