#!/usr/bin/python

from __future__ import unicode_literals
import logging
import os
#from os.path import join
import stat
import hashlib
import argparse
import sqlite3
import operator
import time

###########
# Globals #
###########
table_names = { 'files': 'fp_files', 'dirs': 'fp_dirs', 'roots': 'fp_roots' }

#############
# Functions #
#############
def parse_args():
    """Sets up the argparse object and attempts to parse the command-line.
    """
    
    # Create parser object
    parser = argparse.ArgumentParser(description=
                                     'Tool verify file integrity.')
    parser.add_argument('-l', '--log', nargs='?', 
                        default='', help='log to file instead of console')
    parser.add_argument('-q', '--quiet', action='store_true',
                        help='only display warnings and errors')
    parser.add_argument('-d', '--debug', action='store_true',
                        help='display debugging information')
    parser.add_argument('--db', default='check_files.db',
                        help='Database that contains file fingerprint info.')
    parser.add_argument('--dry-run', action='store_true',
                        help='prints commands to execute, but don\'t do ' +
                        'anything.')
    parser.add_argument('root_dir', nargs='+', 
                        help='Root of a directory tree to scan and check.')

    # Create namespace from command-line
    return parser.parse_args()

def setup_logger(quiet_mode, debug_mode, filename=''):
    """Sets up the global logging object.
    """

    # Determine log level
    if debug_mode:
        logLevel = logging.DEBUG
    elif quiet_mode:
        logLevel = logging.WARNING
    else:
        logLevel = logging.INFO

    # Start logger with default level
    logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s', 
                        datefmt='%Y-%m-%d %I:%M:%S', level=logging.INFO,
                        filename=filename)

    # Denote start of logging, if using a file
    if 0 < len(filename):
        logging.info('Logging started')

    # If logging debug messages, indicate so in file.
    if debug_mode:
        logging.info('Logging debug messages is enabled')

    # Set log level
    logging.getLogger().setLevel(logLevel)

def read_in_chunks(file_obj, chunk_size=1024*1024):
    """Generator to read data from the specified file in chunks.
    Default chunk size is 1M.
    """
    while True:
        data = file_obj.read(chunk_size)
        if not data:
            break
        yield data

def calc_fingerprint(filename):
    """Returns a string containing the SHA1 fingerprint of this file.
    """
    # Create a new SHA1 object and read from the specified file in chunks.
    result = hashlib.sha1()
    with open(filename, 'rb') as f:
        for read_data in read_in_chunks(f):
            result.update(read_data)

    # Return fingerprint
    return result.hexdigest()
    
def process_file(path, parent_id, filename, mode, cursor):
    """Processes the specified file.
    TO DO
    Returns a tuple that can be used to update stats. 
    Order: (added, good, updated, bad)
    """
    # Concatenate path and filename
    fullname = os.path.join(path, filename)

    # Log the file
    logging.info('Processing file \'%s\'', fullname)
    
    # Generate fingerprint
    fp = calc_fingerprint(fullname)

    # Grab entire record from the database
    # TODO: Do we do this once ala "files not seen"?
    cursor.execute("SELECT file_ID,LastModified,Fingerprint,Size from '" 
                   + table_names['files'] + "' WHERE Parent_ID=? and " +
                   "Name=?", (parent_id, filename))
    file_rec = cursor.fetchone()

    # Check current file against one in database
    if file_rec == None:
        # File doesn't exist - add
        cursor.execute("INSERT INTO '" + table_names['files'] + 
                       "'(Name, Parent_ID, LastModified, " +
                       "Fingerprint," +
                       " Size) VALUES(?, ?, ?, ?, ?)", 
                       (filename, parent_id, mode.st_mtime, fp, mode.st_size))
        
        logging.debug('File \'' + path + '\' not in ' +
                      'database. Added with ID = ' + 
                      str(cursor.lastrowid))
        # Return tuple indicating addition
        return (1,0,0,0)
    else:
        # Split out entries of file_rec to local variables to avoid confusion
        db_id = file_rec[0]
        db_mtime = file_rec[1]
        db_fp = file_rec[2]
        db_size = file_rec[3]

        if db_fp != fp:
            # Fingerprints don't match...
            logging.debug('File \'' + fullname + '\' does not match ' +
                          'fingerprint in database')
                    
            # Compare modification times. If this file is newer, 
            # update database. Otherwise issue a warning.
            if float(db_mtime) < mode.st_mtime:
                logging.info('File \'' + fullname + '\' is newer than '
                             + 'database record. Updating...')
                cursor.execute("UPDATE '" + table_names['files'] + 
                               "' SET LastModified=?, " +
                               "Fingerprint=?, Size=? WHERE " +
                               "File_ID=?", (mode.st_mtime, fp, mode.st_size, 
                                             db_id))
                # Return tuple indicating update
                return (0,0,1,0)
            else:
                logging.warning('File \'' + fullname + '\' does not ' +
                                'match fingerprint in database ' +
                                'and is not newer. File could be damaged!')
                # Return tuple indicating problem
                return (0,0,0,1)
        else:
            logging.debug('File \'' + fullname + '\' matches ' +
                          'fingerprint in database')
            # Double-check size to make sure we aren't fooling 
            # ourselves.
            if db_size != mode.st_size:
                logging.warning('File sizes do not match for ' +
                                'file \'' + fullname + 
                                '\'! File could be damaged!')
                # Return tuple indicating problem
                return (0,0,0,1)
            else:
                logging.debug('File \'' + fullname + 
                              '\' matches file size in database')
                # Return tuple indicating file ok
                return (0,1,0,0)

def get_dir_from_db(cursor, dirname, parent_id):
    """Searches the database using the specified cursor object for the first
    record in the directories table with the same name and parent_id. If no
    record exists, one is added. The Path_ID for this directory is returned.
    """

    ret_val = -1

    # Search the database for this path by name and parent_id.
    # If it doesn't exist, add it.
    cursor.execute("SELECT Path_ID from '" + table_names['dirs'] + 
                   "' WHERE Parent_ID=? AND Name=?",
                   (parent_id, dirname))
    row = cursor.fetchone()
    if row == None:
        cursor.execute("INSERT INTO '" + table_names['dirs'] + 
                       "'(Name, Parent_ID) VALUES(?, ?)", 
                       (dirname, str(parent_id)))
        ret_val = cursor.lastrowid
        logging.debug('Directory \'' + root + '\' not in database. Added ' +
                      'with ID = ' + str(ret_val))
    else:
        ret_val = row[0]
        logging.debug('Path \'' + root + '\' found in database with ID = ' +
                      str(ret_val))

    # Return Path_ID for this directory
    return ret_val

def process_root(root, db_conn):
    """Crawls the specified directory, processing all files that it finds.
    """
    
    cursor = db_conn.cursor()

    # begin temp
    path = root
    # end temp

    dirname = os.path.basename(path.rstrip(os.sep))
#    parent_id = get_dir_from_db(cursor, dirname, -1)
#    dir_queue = [(dirname, parent_id)]
    dir_queue = [(dirname, -1)]

    # Order: (added, good, updated, bad)
    file_stats = (0,0,0,0)
    dir_stats = (0,0,0,0)

    # Walk the filesystem
    while 0 < len(dir_queue):
        node = dir_queue.pop()

        # Log the directory that we are processing
        logging.info('Processing directory \'%s\'', node[0])

## This may no longer be necessary since it is handled on original root.
        dirname = os.path.basename(node[0].rstrip(os.sep))

        # Query the database
        dir_id = get_dir_from_db(cursor, dirname, node[1])
    
        # Fetch the list of all files that have this directory as a parent.
### TO DO: Look at retrieving all records for this object and keeping in memory.
### Compare runtimes with the 'one at a time' method.
        cursor.execute("SELECT Name from '" + table_names['files'] + 
                       "' WHERE Parent_ID=?", (dir_id,))
        files_not_seen = cursor.fetchall()

        # Fetch the list of all directories that have this directory as a 
        # parent.
        cursor.execute("SELECT Name from '" + table_names['dirs'] + 
                       "' WHERE Parent_ID=?", (dir_id,))
        dirs_not_seen = cursor.fetchall()

        ## Process directory contents
        for entry in os.listdir(node[0]):
            # Generate full path to entry
            entry_full_name = os.path.join(node[0], entry)

            # Grab entry filesystem stats
            entry_stat = os.stat(entry_full_name)

            if stat.S_ISREG(entry_stat.st_mode):
                # Entry is a regular file.

                # Process file and update stats
                file_stats = map(operator.add, file_stats, 
                                 process_file(node[0], dir_id, entry, 
                                              entry_stat, cursor))

                # Mark as seen
                try:
                    files_not_seen.remove( (entry,) )
                    logging.debug('Marking file \'' + entry_full_name + 
                                  '\' as seen.')
                except ValueError:
                    # File not in list
                    ## TO DO
                    pass

            elif stat.S_ISDIR(entry_stat.st_mode):
                # For directories, push them onto the stack then try to remove 
                # them from the list of dirs not seen. If it isn't in the 
                # database, it will be added next pass.
                dir_queue.append( (entry_full_name, dir_id) )
                
                try:
                    dirs_not_seen.remove( (entry,) )
                    logging.debug('Marking directory \'' + entry_full_name + 
                                  '\' as seen.')
                    
                    # Update stats
                    dir_stats = (dir_stats[0], dir_stats[1] + 1, dir_stats[2], 
                                 dir_stats[3])
                except ValueError:
                    # Item not in list - will be added to database next pass,
                    # but update stats here.
                    dir_stats = (dir_stats[0] + 1, dir_stats[1], dir_stats[2], 
                                 dir_stats[3])

        ## TO DO
        if 0 < len(files_not_seen):
            logging.warning('Files that no longer exist in \'%s\': %s', 
                            node[0], files_not_seen)

        if 0 < len(dirs_not_seen):
            logging.warning('Directories that no longer exist in \'%s\': %s', 
                            node[0], dirs_not_seen)

    # Print stats!
    action_names = ('added', 'good', 'updated', 'BAD or MISSING')
    logging.info('Finished processing root \'' + root + '\': ')
    logging.info('    Files:')
    for idx in range(0,4):
        logging.info('      %s were %s', file_stats[idx], action_names[idx])
    logging.info('    Directories:')
    for idx in range(0,4):
        logging.info('      %s were %s', dir_stats[idx], action_names[idx])

    # Return stats
    return (file_stats, dir_stats)

def open_db(db_url):
    """Function to open the specified SQLite database and return a Connection
    object to it. If the requisite table structure does not exist, it will be
    created.
    """

    # Connect to database, using auto-commit
    conn = sqlite3.connect(database=db_url, isolation_level=None)

    # Look for fingerprints table
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    found = {'files': False, 'dirs': False, 'roots': False }
    for table in cursor.fetchall():
        logging.debug("Found table '" + table[0] + "'")
        for fp_table in table_names:
            if (table[0] == table_names[fp_table]):
                logging.debug("Found " + fp_table + " fingerprint table '" + 
                              table[0] + "'.")
                found[fp_table] = True

    # If not found, create
    if not(found['files']):
        cursor.execute("CREATE TABLE '" + table_names['files'] + 
                       "'(File_ID INTEGER PRIMARY KEY AUTOINCREMENT, " +
                       "Name TEXT, Parent_ID INTEGER, " +
                       "LastModified TEXT, Fingerprint TEXT, Size INTEGER, " +
                       "File_Meta_Type INTEGER, Meta_ID INTEGER)")
        
    if not(found['dirs']):
        cursor.execute("CREATE TABLE '" + table_names['dirs'] + 
                       "'(Path_ID INTEGER PRIMARY KEY AUTOINCREMENT, " +
                       "Name TEXT, Parent_ID INT)")
        
    if not(found['roots']):
        cursor.execute("CREATE TABLE '" + table_names['roots'] + 
                       "'(Root_ID INTEGER PRIMARY KEY AUTOINCREMENT, " +
                       "Name TEXT)")
        
    return conn

########
# Main #
########

# Start timer
cpu_start_time = time.clock()
real_start_time = time.time()

# Parse command-line arguments
cmd_args = parse_args()

# Set up logger
setup_logger(cmd_args.quiet,cmd_args.debug,cmd_args.log)
logging.debug('Command-line arguments: %s', vars(cmd_args))

# Open fingerprint database
with open_db(cmd_args.db) as db_conn:

    # Process the specified directory trees.
    for root in cmd_args.root_dir:
        process_root(root, db_conn)

# Fini!
logging.info('Finished. Total Run Time = %.4f seconds (%.4f CPU seconds)', 
             time.time() - real_start_time, time.clock() - cpu_start_time)
